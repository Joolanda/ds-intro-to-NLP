{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from `SMS Spam Collection v. 1` described as:\n",
    "\n",
    "> a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-encoded messages, tagged according being legitimate (ham) or spam.\n",
    "\n",
    "([source](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load useful libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jopanda/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/jopanda/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\n",
    "    \"data/SMSSpamCollection.txt\",\n",
    "    encoding=\"utf-8\",\n",
    "    header=None,\n",
    "    delimiter=\"\\t\",\n",
    "    names=[\"target\", \"text\"],\n",
    ")\n",
    "\n",
    "# Encoding target variable\n",
    "data[\"target\"] = np.where(data[\"target\"] == \"spam\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm sick !! I'm needy !! I want you !! *pouts*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>0</td>\n",
       "      <td>That's fine, I'll bitch at you about it later ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried to contact ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "2676       0  I'm sick !! I'm needy !! I want you !! *pouts*...\n",
       "1065       0  That's fine, I'll bitch at you about it later ...\n",
       "4001       1  This is the 2nd time we have tried to contact ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample of our data\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 5572 instances of 2 variables.\n",
      "It contains 747 spam messages (13.4% of all).\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset contains {} instances of {} variables.\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "print(\n",
    "    \"It contains {} spam messages ({:.1%} of all).\".format(\n",
    "        data[data.target == 1].shape[0],\n",
    "        data[data.target == 1].shape[0] / data.shape[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of spam SMS: \n",
      "    thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???\n",
      "    Bloomberg -Message center +447797706009 Why wait? Apply for your future http://careers. bloomberg.com\n",
      "\n",
      "Examples of non-spam SMS: \n",
      "    Haha... Sounds crazy, dunno can tahan anot...\n",
      "    Ok thats cool. Its , just off either raglan rd or edward rd. Behind the cricket ground. Gimme ring when ur closeby see you tuesday.\n"
     ]
    }
   ],
   "source": [
    "## Printing random samples of text from both the classes i.e. Spam and non-Spam\n",
    "print(\n",
    "    \"Examples of spam SMS: \\n    {}\\n    {}\".format(\n",
    "        data[data.target == 1].sample(1).text.iloc[0],\n",
    "        data[data.target == 1].sample(1).text.iloc[0],\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"\\nExamples of non-spam SMS: \\n    {}\\n    {}\".format(\n",
    "        data[data.target == 0].sample(1).text.iloc[0],\n",
    "        data[data.target == 0].sample(1).text.iloc[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam classification\n",
    "\n",
    "We will here build a \"vanilla\" classifier, without pouring too many thoughts about what the actual messages, spam or not, look like. To improve your model you can of course have a closer look and investigate the data more in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset between train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[\"text\"], data[\"target\"], random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872     Its going good...no problem..but still need li...\n",
       "831     U have a secret admirer. REVEAL who thinks U R...\n",
       "1273                                                Ok...\n",
       "3314    Huh... Hyde park not in mel ah, opps, got conf...\n",
       "4929    Just hopeing that wasn‘t too pissed up to reme...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "As you have seen, our X variable is just plain text == a string. No classifier can handle it, so we need to make the text accessible to the model. Therefore, we can transform the text so that each word is a separate feature and we count how many times that word occurs in the SMS. We can do this with the scikit-learn [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).  It will convert our text and return a sparse matrix. The vocabulary space of an English text is quite large, while in an SMS you will use only a small subset of words. Therefore saving this feature matrix as a sparse matrix will save memory space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# print(\"X_train_vectorized: \", X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (4179,)\n",
      "Vocabulary length = 7546\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"Vocabulary length = {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in 4179 messages we found 7546 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('00', 0),\n",
       " ('000', 1),\n",
       " ('000pes', 2),\n",
       " ('008704050406', 3),\n",
       " ('0089', 4),\n",
       " ('0121', 5),\n",
       " ('01223585334', 6),\n",
       " ('0125698789', 7),\n",
       " ('02', 8),\n",
       " ('0207', 9),\n",
       " ('02072069400', 10),\n",
       " ('02073162414', 11),\n",
       " ('021', 12),\n",
       " ('03', 13),\n",
       " ('04', 14),\n",
       " ('0430', 15),\n",
       " ('05', 16),\n",
       " ('050703', 17),\n",
       " ('0578', 18),\n",
       " ('06', 19)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our vocabulary list (sorted alphabetically)\n",
    "# Does it look like you expected?\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# We can also print the newly created feature matrix\n",
    "# Note: you see its a sparse matrix with many 0 values. \n",
    "# with .toarray() the compressed sparse matrix form is converted to a normal numpy array\n",
    "print(X_train_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train our first model with the vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.985\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "predict_probab = model.predict_proba(vect.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predict_probab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which words have the highest and lowest coefficients.\n",
    "\n",
    "Think back to the sigmoid function (logistic function). \n",
    "What class are observations assigned to if they contain words with high coefficients?  And to which class are they assigned if they contain words with high negative coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'he' 'amp' 'right']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'ringtone' 'text' 'call' 'chat' 'reply' 'new' 'won' 'stop']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model (from lowest to highest values)\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of our first model was already pretty good (~0.95). Let's see if we can improve this with another transformation of our data. Therefore, we will test the TF-IDF transformation next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "TF-IDF is short for **Term Frequency - Inverse Document Frequency**. \n",
    "\n",
    "It measure how important a word is to a document in a set of texts (in our case all SMS we collected). A frequent word in a document that is also frequent in the corpus is less important to a document than a frequent word in a document that is not frequent in the corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2000', 10),\n",
       " ('250', 11),\n",
       " ('2nd', 12),\n",
       " ('50', 13),\n",
       " ('500', 14),\n",
       " ('5000', 15),\n",
       " ('800', 16),\n",
       " ('able', 17),\n",
       " ('about', 18),\n",
       " ('abt', 19),\n",
       " ('account', 20),\n",
       " ('actually', 21),\n",
       " ('address', 22),\n",
       " ('after', 23),\n",
       " ('afternoon', 24),\n",
       " ('again', 25),\n",
       " ('ah', 26),\n",
       " ('aight', 27),\n",
       " ('all', 28),\n",
       " ('alone', 29)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 15\n",
    "# This means a word should have been used in at least 15 SMS \n",
    "vect = TfidfVectorizer(min_df=15).fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# let's look of some of the words gathered with this method\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear in more than 15 text messages\n",
    "len(sorted(vect.vocabulary_.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which words created the largest tfidf values for the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['2000' 'weekly' 'rate' '16' 'sae' 'await' 'vouchers' 'guaranteed' '1000'\n",
      " 'collection']\n",
      "\n",
      "Largest tfidf: \n",
      "['yup' 'with' 'sure' 'babe' 'heart' 'he' 'thank' 'why' 'happy' 'thanx']\n"
     ]
    }
   ],
   "source": [
    "# save all feature names == words in an array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "#sort for the column names according to highest tfidf value in the column\n",
    "sorted_tfidf_index = X_train_vectorized.toarray().max(0).argsort()\n",
    "\n",
    "# print words with highest and lowest tfidf values\n",
    "print(\"Smallest tfidf:\\n{}\\n\".format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print(\"Largest tfidf: \\n{}\".format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new features with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.988\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict_proba(vect.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 577 features out of 7546 (7546 different words were in our training texts), we still get a high value for the AUC score.\n",
    "Feel free to test different values for the minimum document frequency for the tf-idf vectorizer and see how this affects the model.\n",
    "\n",
    "Again, we can look at the coefficients of our new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'that' 'ok' 'later' 'da' 'how']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'call' 'text' 'free' 'stop' 'uk' 'claim' 'www' 'reply' '150p']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a lot of overlap in the features that received the highest and lowest coefficients compared to the previous model; regardless of how we convert our text into features, these words seem to be important for classifying spam with logistic regression.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data can be more preprocessed before being used as features in a model. We will first use stemming as an approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming reduces a word to its stem. The result is less readable to humans, but makes the text more comparable across observations.\n",
    "\n",
    "For example, the words \"consult\", \"consultant\", \"consulting\", \" consultative\", \"consultants\" have the same stem **\"consult \"**.\n",
    "\n",
    "We will now add stemming as a preprocessing step to our workflow. The nltk PorterStemmer will generate the stems of the words. These features will be used in the CountVectorizer to create a matrix with the number of features (stemmed words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# tfidf_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "# stem_vectorizer = TfidfVectorizer(min_df=15, analyzer = stemmed_words)\n",
    "\n",
    "\n",
    "# Transform X_train\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break the above code cell into steps and show what steps are doing what and why are we doing this overall\n",
    "\n",
    "- The function `build_analyzer()` of `CountVectorizer()` handles the pre-processing, tokenizing and n-grams generation for the text\n",
    "- In the function `stemmed_words()` \n",
    "  - The text is first passed through the `build_analyzer()` and then each word in the text is stemmed to its base form\n",
    "- This whole thing is called with the last step when we call `fit_transform()` on the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the below cell we can see how `build_analyzer()` pre-processes the sample text and tokenize it\n",
    "- And at the last line, the stemmer stems each word in the text to its base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text -  Its going good...no problem..but still need little experience to understand american customer voice...\n",
      "------------------------------\n",
      "Text after passing through build_analyzer -  ['its', 'going', 'good', 'no', 'problem', 'but', 'still', 'need', 'little', 'experience', 'to', 'understand', 'american', 'customer', 'voice']\n",
      "------------------------------\n",
      "Text after stemming -  ['it', 'go', 'good', 'no', 'problem', 'but', 'still', 'need', 'littl', 'experi', 'to', 'understand', 'american', 'custom', 'voic']\n"
     ]
    }
   ],
   "source": [
    "sample_text = X_train[:1]\n",
    "print(\"Sample Text - \", sample_text[872])\n",
    "print(\"-\"*30)\n",
    "print(\"Text after passing through build_analyzer - \", cv_analyzer(sample_text[872]))\n",
    "print(\"-\"*30)\n",
    "print(\"Text after stemming - \",[stemmer.stem(w) for w in cv_analyzer(sample_text[872])])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try uncommenting the tfidf lines in the cell above, so instead of using CountVectorizer you can also use TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.984\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_stemm = LogisticRegression(max_iter=1500)\n",
    "model_stemm.fit(X_train_stem_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model_stemm.predict_proba(stem_vectorizer.transform(X_test))[:,1]\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'hope' 'he' 'that']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'rington' 'text' 'chat' 'new' 'repli' 'won' 'call' 'cost']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(stem_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_stemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see some other words in the features with absolute highest coefficients.\n",
    "The AUC-score of classification is between the scores of our last two text representation attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The same way we used stemming we can also apply lemmatization to our data.\n",
    "Lemmatization reduces variant forms to base form (eg. am, are, is --> be; car, cars, car's, cars' --> car).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# cv_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "# lemm_vectorizer = TfidfVectorizer(min_df=15, analyzer=lemmatize_word)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7091)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lemm_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lemmatization we were able to reduce the features from ca. 7500 to 7100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.909\n",
      "AUC = 0.984\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_lemm = LogisticRegression(max_iter=1500)\n",
    "model_lemm.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predict_probab = model_lemm.predict_proba(lemm_vectorizer.transform(X_test))[:,1]\n",
    "predictions = model_lemm.predict(lemm_vectorizer.transform(X_test))\n",
    "\n",
    "print(\"F1 = {:.3f}\".format(f1_score(y_test, predictions)))\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predict_probab)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'but' 'am' 'he' 'then' 'amp']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'uk' 'ringtone' 'text' 'call' 'new' 'reply' 'chat' 'won' 'free']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(lemm_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_lemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for this last model are very similar to the first model we tested. \n",
    "\n",
    "You can test how well lemmatization in combination with tf-idf is working on our example data. Just remove the `#` at the beginning of the line (don't forget to add `#` to the respective same lines before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our spam classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our classifier. You can also input your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse??? Bloomberg -Message center +447797706009 Why wait? Apply for your future http://careers. bloomberg.com'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your input below\n",
    "# input_text = \"We went yesterday to the beach, call me first. But then also call the other guy pls. Use this number and then the other number\"\n",
    "input_text = \"thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse??? Bloomberg -Message center +447797706009 Why wait? Apply for your future http://careers. bloomberg.com\"\n",
    "\n",
    "# Or use an example for the test set\n",
    "#input_text = X_test.sample(1).iloc[0]\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a spam!\n"
     ]
    }
   ],
   "source": [
    "# You can change the model with model_stemm or model_lemm \n",
    "if model.predict(vect.transform([input_text]))[0] == 1:\n",
    "    print('This is a spam!')\n",
    "else:\n",
    "    print('Not a spam :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (probabilities): [0.04822771 0.9516519  0.03784161 0.01434348 0.00503584]\n",
      "Binary predictions: [0 1 0 0 0]\n",
      "True labels: 4456    0\n",
      "690     1\n",
      "944     0\n",
      "3768    0\n",
      "1189    0\n",
      "Name: target, dtype: int64\n",
      "The hamming loss is 0.0223\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert probabilities to binary predictions\n",
    "binary_preds = (predictions >= 0.5).astype(int)\n",
    "\n",
    "# 2. Inspect the first few values\n",
    "print(\"Predictions (probabilities):\", predictions[:5])\n",
    "print(\"Binary predictions:\", binary_preds[:5])\n",
    "print(\"True labels:\", y_test[:5])\n",
    "\n",
    "# 3. Compute hamming loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print(f\"The hamming loss is {hamming_loss(y_test, binary_preds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777458722182341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, binary_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hamming_loss\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe hamming loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhamming_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    209\u001b[39m         skip_parameter_validation=(\n\u001b[32m    210\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    211\u001b[39m         )\n\u001b[32m    212\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    219\u001b[39m     msg = re.sub(\n\u001b[32m    220\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    223\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2815\u001b[39m, in \u001b[36mhamming_loss\u001b[39m\u001b[34m(y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m   2731\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   2732\u001b[39m     {\n\u001b[32m   2733\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msparse matrix\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2738\u001b[39m )\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhamming_loss\u001b[39m(y_true, y_pred, *, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2740\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute the average Hamming loss.\u001b[39;00m\n\u001b[32m   2741\u001b[39m \n\u001b[32m   2742\u001b[39m \u001b[33;03m    The Hamming loss is the fraction of labels that are incorrectly predicted.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2812\u001b[39m \u001b[33;03m    0.75\u001b[39;00m\n\u001b[32m   2813\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2815\u001b[39m     y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2816\u001b[39m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m   2818\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:112\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    109\u001b[39m     y_type = {\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mClassification metrics can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m targets\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    114\u001b[39m             type_true, type_pred\n\u001b[32m    115\u001b[39m         )\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[32m    119\u001b[39m y_type = y_type.pop()\n",
      "\u001b[31mValueError\u001b[39m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "# this code was not working:\n",
    "# Because you passed continuous probability values to hamming_loss instead of \n",
    "# binary class labels, scikit‑learn rejected the input since the metric can \n",
    "# only handle discrete 0/1 targets, not mixed binary‑and‑continuous data.\n",
    "from sklearn.metrics import hamming_loss\n",
    "print(f'The hamming loss is {hamming_loss(y_test,predictions):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to test your own SMS messages and see which words you can add to change the prediction of a ham message to a spam message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming loss measures the fraction of labels your model gets wrong.\n",
    "That’s it — one clean idea.\n",
    "\n",
    "In binary classification (like yours), it’s simply:\n",
    "how many predictions are incorrect\n",
    "divided by the total number of predictions\n",
    "\n",
    "So a Hamming loss of 0.0223 means your model misclassified 2.23% of samples — or, said differently, it got 97.77% correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
